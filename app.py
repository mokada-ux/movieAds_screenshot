import streamlit as st
import os
import cv2
import whisper
import shutil
import base64
from scenedetect import detect, ContentDetector

# ===============================
# è¨­å®š
# ===============================
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ===============================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ===============================
def format_time(sec):
    m = int(sec // 60)
    s = int(sec % 60)
    return f"{m:02}:{s:02}"


def clear_output():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


# ===============================
# Whisper ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
# ===============================
@st.cache_resource
def load_whisper():
    return whisper.load_model("medium")  # ã“ã“ã§ç²¾åº¦UPï¼ˆsmall â†’ mediumï¼‰


# ===============================
# ã‚·ãƒ¼ãƒ³æŠ½å‡ºï¼ˆæ–°æ–¹å¼ï¼‰
# ===============================
def extract_scenes(video_path):
    scenes = detect(video_path, ContentDetector(threshold=27.0))

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = total_frames / fps

    scene_data = []

    # æœ€åˆã®ã‚·ãƒ¼ãƒ³ãŒ0ç§’ã§å§‹ã¾ã‚‰ãªã„å ´åˆã¯è¿½åŠ 
    if len(scenes) == 0 or scenes[0][0].get_seconds() > 1.0:
        scene_data.append({
            "start": 0.0,
            "end": scenes[0][0].get_seconds() if scenes else duration,
            "img": None,
            "text": "",
        })

    # SceneDetect çµæœ
    for start_time, end_time in scenes:
        scene_data.append({
            "start": start_time.get_seconds(),
            "end": end_time.get_seconds(),
            "img": None,
            "text": "",
        })

    # ã‚¹ã‚¯ã‚·ãƒ§
    for i, sc in enumerate(scene_data):
        capture_time = sc["start"] + 0.5 if (sc["end"] - sc["start"]) > 1 else sc["start"]
        cap.set(cv2.CAP_PROP_POS_MSEC, capture_time * 1000)
        ret, frame = cap.read()

        if ret:
            img_path = os.path.join(OUTPUT_DIR, f"scene_{i:03d}.jpg")
            cv2.imwrite(img_path, frame)
            sc["img"] = img_path

    cap.release()
    return scene_data


# ===============================
# æ›¸ãèµ·ã“ã—
# ===============================
def transcribe(video_path):
    model = load_whisper()
    with st.spinner("Whisper ãŒéŸ³å£°ã‚’è§£æä¸­â€¦"):
        result = model.transcribe(video_path, language="ja")
    return result["segments"]


# ===============================
# ã‚·ãƒ¼ãƒ³ã¨å­—å¹•ã®çµåˆ
# ===============================
def align(scenes, segments):
    for seg in segments:
        mid = (seg["start"] + seg["end"]) / 2
        text = seg["text"]

        matched = False
        for sc in scenes:
            if sc["start"] <= mid < sc["end"]:
                sc["text"] += text + " "
                matched = True
                break

        if not matched:
            scenes[-1]["text"] += text + " "

    return scenes


# ===============================
# HTML ç”Ÿæˆï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« UIï¼‰
# ===============================
def render_scenes(scenes):
    html = """
    <style>
    .scroll-container {
        white-space: nowrap;
        overflow-x: auto;
        padding: 15px;
        border: 1px solid #ddd;
        border-radius: 10px;
    }
    .scene-card {
        display: inline-block;
        width: 250px;
        margin-right: 15px;
        vertical-align: top;
        border-radius: 10px;
        background: #fafafa;
        padding: 10px;
        border: 1px solid #ddd;
    }
    .scene-img {
        width: 100%;
        border-radius: 8px;
    }
    .scene-text {
        font-size: 13px;
        margin-top: 8px;
        white-space: normal;
    }
    </style>
    <div class="scroll-container">
    """

    for sc in scenes:
        if sc["img"] and os.path.exists(sc["img"]):
            with open(sc["img"], "rb") as f:
                img_b64 = base64.b64encode(f.read()).decode()
        else:
            img_b64 = ""

        html += f"""
        <div class="scene-card">
            <img src="data:image/jpeg;base64,{img_b64}" class="scene-img"/>
            <div class="scene-text">{sc['text']}</div>
        </div>
        """

    html += "</div>"
    st.markdown(html, unsafe_allow_html=True)


# ===============================
# ãƒ¡ã‚¤ãƒ³ UI
# ===============================
st.title("ğŸ¬ å‹•ç”» â†’ ã‚·ãƒ¼ãƒ³è§£æ & æ›¸ãèµ·ã“ã—ï¼ˆWhisper ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆï¼‰")

uploaded = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi"])

if uploaded:
    video_path = os.path.join(UPLOAD_DIR, uploaded.name)
    with open(video_path, "wb") as f:
        f.write(uploaded.getbuffer())

    st.success("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        clear_output()

        scenes = extract_scenes(video_path)
        segments = transcribe(video_path)
        scenes = align(scenes, segments)

        st.subheader("ğŸ“¸ ã‚·ãƒ¼ãƒ³ & ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰")
        render_scenes(scenes)

        st.success("è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼")

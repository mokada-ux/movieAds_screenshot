import streamlit as st
import os
import tempfile
import subprocess
import base64
import whisper

# ==============================
# Streamlit è¨­å®š
# ==============================
st.set_page_config(page_title="å‹•ç”»ã‚·ãƒ¼ãƒ³è§£æãƒ„ãƒ¼ãƒ«", layout="wide")
st.markdown("""
<style>
.scene-container {
    display: flex;
    flex-wrap: nowrap;
    overflow-x: auto;
    gap: 20px;
    padding: 10px;
}
.scene-card {
    flex: 0 0 auto;
    width: 260px;
    background: #ffffff10;
    padding: 12px;
    border-radius: 12px;
    border: 1px solid #888;
}
.scene-img {
    width: 100%;
    border-radius: 8px;
    border: 1px solid #666;
}
.scene-time {
    font-size: 14px;
    margin-top: 6px;
    color: #ddd;
}
.scene-text {
    font-size: 15px;
    margin-top: 6px;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¬ å‹•ç”»ã‚·ãƒ¼ãƒ³è§£æãƒ„ãƒ¼ãƒ«ï¼ˆMacBookå‘ã‘ãƒªãƒ©ã‚¤ãƒˆç‰ˆï¼‰")

# ==============================
# å‹•ç”»ç§’æ•°å–å¾—
# ==============================
def get_video_duration(video_path):
    cmd = [
        "ffprobe",
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        video_path
    ]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return float(result.stdout.decode().strip())

# ==============================
# ã‚·ãƒ¼ãƒ³æŠ½å‡ºï¼ˆFFmpegï¼‰
# ==============================
def extract_scenes_ffmpeg(video_path):
    tmp_dir = tempfile.mkdtemp()
    threshold = "0.3"
    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-vf",
        f"select='gt(scene,{threshold})',metadata=print",
        "-vsync", "vfr",
        os.path.join(tmp_dir, "scene_%04d.jpg")
    ]
    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    image_paths = sorted(
        [os.path.join(tmp_dir, f) for f in os.listdir(tmp_dir) if f.endswith(".jpg")]
    )
    return image_paths

# ==============================
# Whisper-small éŸ³å£°è§£æ
# ==============================
@st.cache_resource
def load_whisper():
    return whisper.load_model("small")

def transcribe_audio(video_path):
    model = load_whisper()
    result = model.transcribe(video_path, fp16=False, language="ja")
    return result["text"]

# ==============================
# TSVï¼ˆæ¨ª3è¡ŒÃ—nåˆ—ï¼‰
# ==============================
def generate_tsv_horizontal(image_paths, times, transcripts):
    time_row = ["æ™‚é–“"] + [str(t) for t in times]
    image_row = ["ç”»åƒ"]
    for img in image_paths:
        with open(img, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        formula = f'=IMAGE("data:image/jpeg;base64,{b64}")'
        image_row.append(formula)
    text_row = ["ãƒ†ã‚­ã‚¹ãƒˆ"] + transcripts
    return "\n".join([
        "\t".join(time_row),
        "\t".join(image_row),
        "\t".join(text_row)
    ])

# ==============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==============================
uploaded = st.file_uploader(
    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmp4 / movï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov"]
)

if uploaded:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded.read())
        video_path = tmp.name

    st.success("å‹•ç”»ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")

    duration = get_video_duration(video_path)
    st.write(f"å‹•ç”»ã®é•·ã•ï¼š{duration:.1f} ç§’")

    with st.spinner("ã‚·ãƒ¼ãƒ³æŠ½å‡ºä¸­â€¦"):
        scene_images = extract_scenes_ffmpeg(video_path)
    st.write(f"æŠ½å‡ºã‚·ãƒ¼ãƒ³æ•°ï¼š{len(scene_images)}")

    # ã‚·ãƒ¼ãƒ³ç§’æ•°ï¼ˆå‡ç­‰å‰²ã‚Šï¼‰
    times = []
    for i, img in enumerate(scene_images):
        sec = round(i * (duration / max(1, len(scene_images))), 1)
        times.append(sec)

    # Whisper-small
    with st.spinner("éŸ³å£°è§£æä¸­â€¦"):
        transcript = transcribe_audio(video_path)

    # å‡ç­‰ã«åˆ†å‰²
    words = transcript.split()
    chunk = len(scene_images)
    transcripts = []
    if chunk > 0:
        split_size = max(1, len(words) // chunk)
        for i in range(chunk):
            part = words[i * split_size:(i + 1) * split_size]
            transcripts.append(" ".join(part))

    # ==============================
    # UIï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚«ãƒ¼ãƒ‰ï¼‰
    # ==============================
    st.subheader("ğŸ” è‡ªå‹•æŠ½å‡ºã‚·ãƒ¼ãƒ³ï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰")
    html = '<div class="scene-container">'
    for img, t, tx in zip(scene_images, times, transcripts):
        with open(img, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        html += f"""
        <div class="scene-card">
            <img class="scene-img" src="data:image/jpeg;base64,{b64}">
            <div class="scene-time">â± {t} ç§’</div>
            <div class="scene-text">{tx}</div>
        </div>
        """
    html += "</div>"
    st.markdown(html, unsafe_allow_html=True)

    # ==============================
    # TSVå‡ºåŠ›
    # ==============================
    st.subheader("ğŸ“‹ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ TSVï¼ˆæ¨ª3è¡ŒÃ—nåˆ—ï¼‰")
    if st.button("TSV ã‚’ç”Ÿæˆ"):
        tsv = generate_tsv_horizontal(scene_images, times, transcripts)
        st.code(tsv, language="text")
        st.success("ãã®ã¾ã¾ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è²¼ã‚Šä»˜ã‘å¯èƒ½ï¼")

import streamlit as st
import os
import cv2
import whisper
import shutil
import base64
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

# ====== è¨­å®š ====== #
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

st.set_page_config(page_title="å‹•ç”»è§£æ Pro", layout="wide")

# ====== CSS ã‚«ã‚¹ã‚¿ãƒ  ====== #
st.markdown("""
<style>
/* å…¨ä½“ãƒ•ã‚©ãƒ³ãƒˆ */
html, body, [class*="css"]  {
    font-family: "Inter", "Noto Sans JP", sans-serif;
}

/* æ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚³ãƒ³ãƒ†ãƒŠ */
.scene-container {
    display: flex;
    flex-direction: row;
    overflow-x: auto;
    gap: 20px;
    padding-bottom: 20px;
    white-space: nowrap;
}

/* 1ã‚·ãƒ¼ãƒ³ã®ã‚«ãƒ¼ãƒ‰ãƒ‡ã‚¶ã‚¤ãƒ³ */
.scene-card {
    display: inline-block;
    width: 280px;
    background: #ffffff10;
    backdrop-filter: blur(6px);
    padding: 12px;
    border-radius: 14px;
    box-shadow: 0px 4px 14px rgba(0,0,0,0.1);
    border: 1px solid rgba(255,255,255,0.15);
}

/* ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒ */
.scene-img {
    width: 100%;
    border-radius: 10px;
    margin-bottom: 8px;
    border: 1px solid #ddd;
}

/* ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸ */
.scene-text {
    font-size: 14px;
    line-height: 1.5;
    white-space: pre-wrap;
}
</style>
""", unsafe_allow_html=True)


# ====== é–¢æ•°é¡ ====== #

def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def format_time(seconds):
    seconds = int(seconds)
    m = seconds // 60
    s = seconds % 60
    return f"{m:02}:{s:02}"


def extract_scenes(video_path):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=27.0))

    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps

    scenes = []
    if not scene_list or scene_list[0][0].get_seconds() > 1.0:
        scenes.append({"start": 0.0, "end": scene_list[0][0].get_seconds() if scene_list else duration})

    for s in scene_list:
        scenes.append({"start": s[0].get_seconds(), "end": s[1].get_seconds()})

    # ç”»åƒä¿å­˜
    for i, sc in enumerate(scenes):
        cap.set(cv2.CAP_PROP_POS_MSEC, int((sc["start"] + 0.3) * 1000))
        ret, frame = cap.read()
        if ret:
            filename = f"scene_{i:03}.jpg"
            cv2.imwrite(os.path.join(OUTPUT_DIR, filename), frame)
            sc["img"] = os.path.join(OUTPUT_DIR, filename)

        sc["time_str"] = format_time(sc["start"])

    cap.release()
    return scenes


@st.cache_resource
def load_whisper():
    return whisper.load_model("small")   # â†ç²¾åº¦UP


def transcribe_audio(path):
    model = load_whisper()
    result = model.transcribe(path, language="ja")
    return result["segments"]


def align(scenes, segments):
    for sc in scenes:
        sc["text"] = ""

    for seg in segments:
        mid = (seg["start"] + seg["end"]) / 2
        for sc in scenes:
            if sc["start"] <= mid < sc["end"]:
                sc["text"] += seg["text"] + "\n"
                break
        else:
            scenes[-1]["text"] += seg["text"] + "\n"

    return scenes


# ====== UI ====== #

st.title("ğŸ¥ å‹•ç”»è§£æ Proï¼ˆUIå¼·åŒ–ç‰ˆï¼‰")
uploaded = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi"])

if uploaded is not None:
    video_path = os.path.join(UPLOAD_DIR, uploaded.name)
    with open(video_path, "wb") as f:
        f.write(uploaded.getbuffer())

    st.success("å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary"):
        clear_output_folder()

        with st.spinner("ã‚·ãƒ¼ãƒ³æŠ½å‡ºä¸­..."):
            scenes = extract_scenes(video_path)

        with st.spinner("éŸ³å£°è§£æä¸­...ï¼ˆWhisper smallï¼‰"):
            segments = transcribe_audio(video_path)

        scenes = align(scenes, segments)

        st.subheader("ğŸ¬ ã‚·ãƒ¼ãƒ³ä¸€è¦§ï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯ï¼‰")

        # ===== æ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« HTMLç”Ÿæˆ ===== #
        html = """<div class="scene-container">"""

        for sc in scenes:
            with open(sc["img"], "rb") as f:
                encoded = base64.b64encode(f.read()).decode()

            html += f"""
            <div class="scene-card">
                <img src="data:image/jpeg;base64,{encoded}" class="scene-img" />
                <div><b>â± {sc['time_str']}ã€œ</b></div>
                <div class="scene-text">{sc['text']}</div>
            </div>
            """

        html += "</div>"

        st.markdown(html, unsafe_allow_html=True)

        st.subheader("ğŸ“Š ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ï¼ˆæ¨ªä¸¦ã³ï¼‰")

        tsv = "\t".join([s["text"].replace("\n", " ") for s in scenes])
        st.code(tsv, language="text")

import streamlit as st
import os
import cv2
import whisper
import shutil
import datetime
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

# --- è¨­å®š ---
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: æ™‚é–“è¡¨ç¤º ---
def format_time(seconds):
    seconds = int(seconds)
    minutes = seconds // 60
    rem_seconds = seconds % 60
    return f"{minutes:02}:{rem_seconds:02}"

# --- é–¢æ•°: ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚»ãƒƒãƒˆ ---
def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: ã‚·ãƒ¼ãƒ³æŠ½å‡º ---
def extract_scenes(video_path):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    # threshold=27.0 ã¯æ¨™æº–ã€‚å‹•ããŒæ¿€ã—ã„å‹•ç”»ã§ç´°åˆ‡ã‚Œã«ãªã‚‹å ´åˆã¯35.0ãã‚‰ã„ã«ä¸Šã’ã‚‹
    scene_manager.add_detector(ContentDetector(threshold=27.0))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()
    
    cap = cv2.VideoCapture(video_path)
    # å‹•ç”»ã®ç·å†ç”Ÿæ™‚é–“ã‚’å–å¾—
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps
    
    scenes_data = []
    
    # æœ€åˆã®ã‚·ãƒ¼ãƒ³(0ç§’åœ°ç‚¹)ã‚’å¼·åˆ¶è¿½åŠ ã™ã‚‹ã‹åˆ¤å®š
    start_time_offset = 0.0
    if not scene_list or scene_list[0][0].get_seconds() > 1.0:
        scenes_data.append({
            "start": 0.0,
            "end": scene_list[0][0].get_seconds() if scene_list else duration,
            "time_str": format_time(0),
            "img_path": None # å¾Œã§æ’®å½±
        })

    # ã‚·ãƒ¼ãƒ³ãƒªã‚¹ãƒˆã‚’æ•´å½¢
    for i, scene in enumerate(scene_list):
        start = scene[0].get_seconds()
        end = scene[1].get_seconds()
        scenes_data.append({
            "start": start,
            "end": end,
            "time_str": format_time(start),
            "img_path": None
        })
    
    # ç”»åƒä¿å­˜å‡¦ç†
    progress_bar = st.progress(0, text="ã‚·ãƒ¼ãƒ³ç”»åƒã‚’æŠ½å‡ºä¸­...")
    total_scenes = len(scenes_data)
    
    for i, data in enumerate(scenes_data):
        # ã‚·ãƒ¼ãƒ³é–‹å§‹ç›´å¾Œã ã¨ãƒ–ãƒ¬ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€0.5ç§’å¾Œãªã©ã‚’å–å¾—ã—ã¦ã¿ã‚‹
        # ãŸã ã—ã‚·ãƒ¼ãƒ³ãŒçŸ­ã™ãã‚‹å ´åˆã¯é–‹å§‹æ™‚ç‚¹ã‚’ä½¿ã†
        scene_len = data["end"] - data["start"]
        capture_point = data["start"] + (0.5 if scene_len > 1.0 else 0.0)
        
        cap.set(cv2.CAP_PROP_POS_MSEC, capture_point * 1000)
        ret, frame = cap.read()
        
        if ret:
            img_filename = f"scene_{i:03d}.jpg"
            img_path = os.path.join(OUTPUT_DIR, img_filename)
            cv2.imwrite(img_path, frame)
            scenes_data[i]["img_path"] = img_path
        
        progress_bar.progress(min((i + 1) / total_scenes, 1.0))

    cap.release()
    progress_bar.empty()
    return scenes_data

# --- é–¢æ•°: éŸ³å£°æ›¸ãèµ·ã“ã— ---
@st.cache_resource
def load_whisper_model():
    # ç²¾åº¦é‡è¦–ãªã‚‰ small, æ›´ã«ä¸Šã’ã‚‹ãªã‚‰ medium
    return whisper.load_model("small")

def transcribe_audio(video_path):
    model = load_whisper_model()
    with st.spinner("AIãŒéŸ³å£°ã‚’è§£æã—ã¦ã„ã¾ã™..."):
        result = model.transcribe(video_path, language="ja")
    return result["segments"]

# --- é–¢æ•°: ç²¾åº¦å‘ä¸Šç‰ˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆä¸­ç‚¹ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ ---
def align_scenes_and_text(scenes, segments):
    # ã‚·ãƒ¼ãƒ³ã”ã¨ã«ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’ç”¨æ„
    for scene in scenes:
        scene["text_list"] = []

    for segment in segments:
        # ã‚»ãƒªãƒ•ã®é–‹å§‹ãƒ»çµ‚äº†ãƒ»ä¸­é–“ç‚¹
        seg_start = segment["start"]
        seg_end = segment["end"]
        seg_mid = (seg_start + seg_end) / 2 # â˜…ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ

        # ã€Œã‚»ãƒªãƒ•ã®ä¸­é–“ç‚¹ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚·ãƒ¼ãƒ³ã‚’æ¢ã™
        matched = False
        for scene in scenes:
            # æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã®endæ™‚é–“ãŒæ›–æ˜§ãªå ´åˆã®ã‚¬ãƒ¼ãƒ‰ãªã©ã‚’è€ƒæ…®ã—ã¤ã¤åˆ¤å®š
            if scene["start"] <= seg_mid < scene["end"]:
                scene["text_list"].append(segment["text"])
                matched = True
                break
        
        # ã©ã®ã‚·ãƒ¼ãƒ³ã«ã‚‚å…¥ã‚‰ãªã‹ã£ãŸå ´åˆï¼ˆå‹•ç”»æœ€å¾Œã®ä½™éŸ»ãªã©ï¼‰ã€æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã«å…¥ã‚Œã‚‹
        if not matched and scenes:
             scenes[-1]["text_list"].append(segment["text"])

    # ãƒªã‚¹ãƒˆã‚’çµåˆã—ã¦æ–‡å­—åˆ—ã«ã™ã‚‹
    for scene in scenes:
        scene["final_text"] = "\n".join(scene["text_list"])
    
    return scenes

# ==========================================
# ãƒ¡ã‚¤ãƒ³UI
# ==========================================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro v2", layout="wide")

st.title("ğŸ¥ å‹•ç”»è§£æ & ã‚¹ãƒ—ã‚·è²¼ã‚Šä»˜ã‘ãƒ„ãƒ¼ãƒ«")
st.markdown("ã‚·ãƒ¼ãƒ³æ¤œå‡ºã®ç²¾åº¦å‘ä¸Šã¨ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ¨ªä¸¦ã³è²¼ã‚Šä»˜ã‘ã«å¯¾å¿œã—ã¾ã—ãŸã€‚")

uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi"])

if uploaded_file is not None:
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary"):
        clear_output_folder()
        
        # 1. å®Ÿè¡Œ
        scenes = extract_scenes(video_path)
        segments = transcribe_audio(video_path)
        
        # 2. çµåˆï¼ˆç²¾åº¦å‘ä¸Šãƒ­ã‚¸ãƒƒã‚¯é©ç”¨ï¼‰
        aligned_data = align_scenes_and_text(scenes, segments)
        
        st.divider()

        # --- è¡¨ç¤ºã‚¨ãƒªã‚¢ ---
        st.subheader("1. è§£æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        # 3åˆ—ã”ã¨ã«æŠ˜ã‚Šè¿”ã—ã¦è¡¨ç¤º
        cols = st.columns(3)
        for i, item in enumerate(aligned_data):
            with cols[i % 3]:
                if item["img_path"]:
                    st.image(item["img_path"], use_column_width=True)
                st.caption(f"ã‚·ãƒ¼ãƒ³ {i+1} ({item['time_str']}~)")
                st.text_area("å†…å®¹", item["final_text"], height=100, key=f"t_{i}")

        st.divider()

        # --- ã‚¹ãƒ—ã‚·ç”¨ã‚³ãƒ”ãƒ¼ã‚¨ãƒªã‚¢ ---
        st.subheader("2. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ç”¨ãƒ‡ãƒ¼ã‚¿")
        st.markdown("""
        ä»¥ä¸‹ã®ãƒœãƒƒã‚¯ã‚¹ã®å³ä¸Šã«ã‚ã‚‹ **ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³** ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚  
        ãã®å¾Œã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã‚»ãƒ«ã‚’é¸ã‚“ã§è²¼ã‚Šä»˜ã‘ã‚‹ã¨ã€**æ¨ªä¸€åˆ—ã«ã‚·ãƒ¼ãƒ³ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã‚Šã¾ã™ã€‚**
        """)

        # ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ†ã‚­ã‚¹ãƒˆ(TSV)ã‚’ä½œæˆ
        # joinã™ã‚‹ã¨ãã«ã‚¿ãƒ–(\t)ã‚’ä½¿ã†ã“ã¨ã§ã€ã‚¨ã‚¯ã‚»ãƒ«ç­‰ã¯ã€Œéš£ã®ã‚»ãƒ«ã€ã¨èªè­˜ã—ã¾ã™
        tsv_text = "\t".join([item["final_text"].replace("\n", " ") for item in aligned_data])
        
        # st.codeã‚’ä½¿ã£ã¦ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ä»˜ãã®ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º
        st.code(tsv_text, language="text")

import streamlit as st
import os
import cv2
import whisper
import shutil
import datetime
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
# pandasã¯ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã®ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™
import pandas as pd

# --- è¨­å®š ---
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: æ™‚é–“è¡¨ç¤º ---
def format_time(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

# --- é–¢æ•°: ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚»ãƒƒãƒˆ ---
def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: ã‚·ãƒ¼ãƒ³æŠ½å‡º ---
def extract_scenes(video_path):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    # threshold=27.0 ã¯æ„Ÿåº¦ã®æ¨™æº–å€¤ã€‚
    scene_manager.add_detector(ContentDetector(threshold=27.0))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()
    
    cap = cv2.VideoCapture(video_path)
    scenes_data = []

    progress_bar = st.progress(0, text="ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­...")
    total_scenes = len(scene_list)
    
    # æœ€åˆã®ã‚·ãƒ¼ãƒ³ã®é–‹å§‹æ™‚é–“ã¯å¿…ãš0ç§’ã¨ã™ã‚‹
    if total_scenes > 0 and scene_list[0][0].get_seconds() > 0:
         start_time = 0.0
         cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
         ret, frame = cap.read()
         if ret:
             img_filename = f"scene_start_0s.jpg"
             img_path = os.path.join(OUTPUT_DIR, img_filename)
             cv2.imwrite(img_path, frame)
             scenes_data.append({
                 "time_str": format_time(start_time),
                 "seconds": start_time,
                 "img_path": img_path
             })

    for i, scene in enumerate(scene_list):
        start_time = scene[0].get_seconds()
        
        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
        ret, frame = cap.read()
        
        if ret:
            img_filename = f"scene_{i:03d}_{int(start_time)}s.jpg"
            img_path = os.path.join(OUTPUT_DIR, img_filename)
            cv2.imwrite(img_path, frame)
            
            scenes_data.append({
                "time_str": format_time(start_time),
                "seconds": start_time,
                "img_path": img_path
            })
        
        if total_scenes > 0:
            progress_bar.progress(min((i + 1) / total_scenes, 1.0))

    cap.release()
    progress_bar.empty()
    return scenes_data

# --- é–¢æ•°: éŸ³å£°æ›¸ãèµ·ã“ã—ï¼ˆç²¾åº¦å‘ä¸Šç‰ˆï¼‰ ---
@st.cache_resource
def load_whisper_model():
    # â˜…å¤‰æ›´ç‚¹ï¼šç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ "base" ã‹ã‚‰ "small" ã«å¤‰æ›´
    # ã‚¯ãƒ©ã‚¦ãƒ‰ã§è½ã¡ã‚‹å ´åˆã¯ "base" ã«æˆ»ã—ã¦ãã ã•ã„ã€‚
    # ãƒ­ãƒ¼ã‚«ãƒ«ã§ä½™è£•ãŒã‚ã‚Œã° "medium" ã‚‚å¯ã€‚
    return whisper.load_model("small") 

def transcribe_audio(video_path):
    model = load_whisper_model()
    with st.spinner("AIãŒéŸ³å£°ã‚’è§£æã—ã¦ã„ã¾ã™... (ãƒ¢ãƒ‡ãƒ«ã‚’å¤§ããã—ãŸãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)"):
        # language="ja" ã‚’æŒ‡å®šã™ã‚‹ã¨èªè­˜ç‡ãŒä¸ŠãŒã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
        result = model.transcribe(video_path, language="ja")
    return result["segments"]

# --- é–¢æ•°: ãƒ‡ãƒ¼ã‚¿çµåˆï¼ˆä»Šå›ã®è‚ï¼‰ ---
def align_scenes_and_text(scenes, segments):
    aligned_data = []
    
    for i, scene in enumerate(scenes):
        scene_start = scene["seconds"]
        # æ¬¡ã®ã‚·ãƒ¼ãƒ³ã®é–‹å§‹æ™‚é–“ã‚’å–å¾—ï¼ˆæœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã®å ´åˆã¯ç„¡é™å¤§ã‚’è¨­å®šï¼‰
        next_scene_start = scenes[i+1]["seconds"] if i+1 < len(scenes) else float('inf')
        
        # ã“ã®ã‚·ãƒ¼ãƒ³ã®åŒºé–“å†…ã«é–‹å§‹æ™‚é–“ãŒã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ¢ã™
        matched_texts = []
        for segment in segments:
            if scene_start <= segment["start"] < next_scene_start:
                matched_texts.append(segment["text"])
        
        # è¤‡æ•°è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆã‚¹ãƒ—ã‚·ã§è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
        combined_text = "\n".join(matched_texts)
        
        aligned_data.append({
            "time": scene["time_str"],
            "image": scene["img_path"],
            "text": combined_text
        })
    return aligned_data

# ==========================================
# ãƒ¡ã‚¤ãƒ³UI
# ==========================================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒªPro", layout="wide")

st.title("ğŸ¥ å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro (ã‚¹ãƒ—ã‚·å¯¾å¿œç‰ˆ)")
st.markdown("""
- **ç²¾åº¦å‘ä¸Š:** éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’é«˜æ€§èƒ½ãªã‚‚ã®ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚
- **ã‚¹ãƒ—ã‚·å¯¾å¿œ:** ã‚·ãƒ¼ãƒ³ç”»åƒã®ä¸‹ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é…ç½®ã—ã¾ã™ã€‚ãã®ã¾ã¾ã‚³ãƒ”ãƒšã§ãã¾ã™ã€‚
""")

uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi", "mkv"])

if uploaded_file is not None:
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"èª­ã¿è¾¼ã¿å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ (å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)", type="primary"):
        clear_output_folder()
        
        # 1. å‡¦ç†å®Ÿè¡Œ
        scenes = extract_scenes(video_path)
        segments = transcribe_audio(video_path)
        
        # 2. ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®çªãåˆã‚ã›
        aligned_data = align_scenes_and_text(scenes, segments)
        
        st.divider()
        st.subheader("ğŸ“Š è§£æçµæœ (ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ)")
        st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ç”»åƒã®è¡Œã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã®è¡Œã¾ã§ãƒ‰ãƒ©ãƒƒã‚°ã—ã¦é¸æŠã—ã€Excelã‚„Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚")

        if not aligned_data:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            # --- ã‚¹ãƒ—ã‚·ç”¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¡¨ç¤º ---
            # Streamlitã§æ¨ªä¸¦ã³ã‚’ç¶ºéº—ã«ã‚³ãƒ”ãƒšã•ã›ã‚‹ãŸã‚ã€å°‘ã—ç‰¹æ®Šãªè¡¨ç¤ºã‚’ã—ã¾ã™ã€‚
            
            num_scenes = len(aligned_data)
            
            # 1è¡Œç›®ï¼šæ™‚é–“è¡¨ç¤º
            cols_time = st.columns(num_scenes)
            for i, col in enumerate(cols_time):
                col.write(f"**{aligned_data[i]['time']}**")
            
            # 2è¡Œç›®ï¼šç”»åƒè¡¨ç¤º
            cols_img = st.columns(num_scenes)
            for i, col in enumerate(cols_img):
                col.image(aligned_data[i]["image"], use_column_width=True)
                
            # 3è¡Œç›®ï¼šãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º (ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‚’ä½¿ã†ã¨ã‚³ãƒ”ãƒšã—ã‚„ã™ã„)
            cols_text = st.columns(num_scenes)
            for i, col in enumerate(cols_text):
                # heightèª¿æ•´ã§è¦‹ãŸç›®ã‚’æƒãˆã‚‹
                col.text_area("ãƒ†ã‚­ã‚¹ãƒˆ", aligned_data[i]["text"], height=150, label_visibility="hidden", key=f"text_{i}")

            st.divider()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
            df = pd.DataFrame(aligned_data)
            csv = df.to_csv(index=False).encode('utf-8_sig')
            st.download_button(
                "ğŸ“¥ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv,
                "video_analysis.csv",
                "text/csv",
                 key='download-csv'
            )

import streamlit as st
import os
import cv2
import whisper
import shutil
import datetime
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
import pandas as pd

# --- è¨­å®š ---
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: æ™‚é–“è¡¨ç¤º ---
def format_time(seconds):
    return str(datetime.timedelta(seconds=int(seconds)))

# --- é–¢æ•°: ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚»ãƒƒãƒˆ ---
def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: ã‚·ãƒ¼ãƒ³æŠ½å‡º ---
def extract_scenes(video_path):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=27.0))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()
    
    cap = cv2.VideoCapture(video_path)
    scenes_data = []

    progress_bar = st.progress(0, text="ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­...")
    total_scenes = len(scene_list)

    # æœ€åˆã®ã‚·ãƒ¼ãƒ³(é–‹å§‹0ç§’)ã‚’å¿…ãšè¿½åŠ 
    if total_scenes > 0 and scene_list[0][0].get_seconds() > 0:
        start_time = 0.0
        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
        ret, frame = cap.read()
        if ret:
            img_filename = "scene_000_0s.jpg"
            img_path = os.path.join(OUTPUT_DIR, img_filename)
            cv2.imwrite(img_path, frame)
            scenes_data.append({
                "time_str": format_time(start_time),
                "seconds": start_time,
                "img_path": img_path
            })

    # é€šå¸¸ã‚·ãƒ¼ãƒ³
    for i, scene in enumerate(scene_list):
        start_time = scene[0].get_seconds()
        
        cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)
        ret, frame = cap.read()
        
        if ret:
            img_filename = f"scene_{i:03d}_{int(start_time)}s.jpg"
            img_path = os.path.join(OUTPUT_DIR, img_filename)
            cv2.imwrite(img_path, frame)
            
            scenes_data.append({
                "time_str": format_time(start_time),
                "seconds": start_time,
                "img_path": img_path
            })
        
        if total_scenes > 0:
            progress_bar.progress(min((i + 1) / total_scenes, 1.0))

    cap.release()
    progress_bar.empty()
    return scenes_data

# --- Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("small")

def transcribe_audio(video_path):
    model = load_whisper_model()
    with st.spinner("éŸ³å£°ã‚’è§£æä¸­..."):
        result = model.transcribe(video_path, language="ja")
    return result["segments"]

# --- ã‚·ãƒ¼ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆçµåˆ ---
def align_scenes_and_text(scenes, segments):
    aligned_data = []
    
    for i, scene in enumerate(scenes):
        scene_start = scene["seconds"]
        next_scene_start = scenes[i+1]["seconds"] if i+1 < len(scenes) else float('inf')
        
        matched_texts = [
            seg["text"] for seg in segments
            if scene_start <= seg["start"] < next_scene_start
        ]
        
        combined_text = "\n".join(matched_texts)
        
        aligned_data.append({
            "time": scene["time_str"],
            "image": scene["img_path"],
            "text": combined_text
        })
    return aligned_data


# ==========================================
# UI
# ==========================================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro", layout="wide")

st.title("ğŸ¥ å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro")
st.markdown("Geminiç‰ˆã¨åŒã˜æŒ™å‹•ã§å‹•ãã‚ˆã†ã«æœ€é©åŒ–æ¸ˆã¿ã€‚")

# MIMEåˆ¶é™ã‚’å¤–ã—ã¦å‹•ç”»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹
uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", accept_multiple_files=False)

if uploaded_file is not None:
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"èª­ã¿è¾¼ã¿å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        clear_output_folder()

        scenes = extract_scenes(video_path)
        segments = transcribe_audio(video_path)
        aligned_data = align_scenes_and_text(scenes, segments)

        st.divider()
        st.subheader("ğŸ“Š è§£æçµæœï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ç”¨ï¼‰")

        num_scenes = len(aligned_data)

        # æ™‚é–“
        cols_time = st.columns(num_scenes)
        for i, col in enumerate(cols_time):
            col.write(f"**{aligned_data[i]['time']}**")

        # ç”»åƒ
        cols_img = st.columns(num_scenes)
        for i, col in enumerate(cols_img):
            col.image(aligned_data[i]["image"], use_column_width=True)

        # ãƒ†ã‚­ã‚¹ãƒˆ
        cols_text = st.columns(num_scenes)
        for i, col in enumerate(cols_text):
            col.text_area("", aligned_data[i]["text"], height=150, key=f"t_{i}")

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        df = pd.DataFrame(aligned_data)
        csv = df.to_csv(index=False).encode("utf-8_sig")
        st.download_button("ğŸ“¥ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv, "video_analysis.csv")

import streamlit as st
import os
import cv2
import whisper
import shutil
import datetime
# pandasã¯ãƒ‡ãƒ¼ã‚¿æ•´å½¢ç”¨
import pandas as pd
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

# --- è¨­å®š ---
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: æ™‚é–“è¡¨ç¤º ---
def format_time(seconds):
    seconds = int(seconds)
    minutes = seconds // 60
    rem_seconds = seconds % 60
    return f"{minutes:02}:{rem_seconds:02}"

# --- é–¢æ•°: ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚»ãƒƒãƒˆ ---
def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- é–¢æ•°: ã‚·ãƒ¼ãƒ³æŠ½å‡º ---
def extract_scenes(video_path):
    # ã‚·ãƒ¼ãƒ³æ¤œå‡ºå™¨ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    # å‹•ãã®æ„Ÿåº¦è¨­å®šï¼ˆæ•°å­—ãŒå¤§ãã„ã»ã©æ•æ„Ÿï¼‰
    scene_manager.add_detector(ContentDetector(threshold=27.0))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()
    
    # ç”»åƒä¿å­˜ã®æº–å‚™
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps if fps > 0 else 0
    
    scenes_data = []
    
    # ã‚·ãƒ¼ãƒ³ãƒªã‚¹ãƒˆãŒç©ºã®å ´åˆã®ä¿é™ºï¼ˆå‹•ç”»å…¨ä½“ã‚’1ã‚·ãƒ¼ãƒ³ã¨ã™ã‚‹ï¼‰
    if not scene_list:
        scenes_data.append({
            "start": 0.0,
            "end": duration,
            "time_str": format_time(0),
            "img_path": None
        })
    else:
        # æœ€åˆã®ã‚·ãƒ¼ãƒ³ãŒ0ç§’ã‹ã‚‰å§‹ã¾ã£ã¦ã„ãªã„å ´åˆã®è£œæ­£
        if scene_list[0][0].get_seconds() > 1.0:
            scenes_data.append({
                "start": 0.0,
                "end": scene_list[0][0].get_seconds(),
                "time_str": format_time(0),
                "img_path": None
            })
        
        for scene in scene_list:
            start = scene[0].get_seconds()
            end = scene[1].get_seconds()
            scenes_data.append({
                "start": start,
                "end": end,
                "time_str": format_time(start),
                "img_path": None
            })
    
    # ç”»åƒã‚­ãƒ£ãƒ—ãƒãƒ£å‡¦ç†
    progress_bar = st.progress(0, text="ã‚·ãƒ¼ãƒ³ç”»åƒã‚’æŠ½å‡ºä¸­...")
    total_scenes = len(scenes_data)
    
    for i, data in enumerate(scenes_data):
        # ã‚·ãƒ¼ãƒ³é–‹å§‹ç›´å¾Œã‚ˆã‚Šå°‘ã—å¾Œï¼ˆ0.5ç§’å¾Œï¼‰ã‚’æ’®ã‚‹ã“ã¨ã§ãƒ–ãƒ¬ã‚’é˜²ã
        capture_point = data["start"] + 0.5
        if capture_point >= data["end"]:
            capture_point = data["start"] # ã‚·ãƒ¼ãƒ³ãŒçŸ­ã™ãã‚‹å ´åˆã¯é–‹å§‹ç‚¹
            
        cap.set(cv2.CAP_PROP_POS_MSEC, capture_point * 1000)
        ret, frame = cap.read()
        
        if ret:
            img_filename = f"scene_{i:03d}.jpg"
            img_path = os.path.join(OUTPUT_DIR, img_filename)
            cv2.imwrite(img_path, frame)
            scenes_data[i]["img_path"] = img_path
        
        if total_scenes > 0:
            progress_bar.progress(min((i + 1) / total_scenes, 1.0))

    cap.release()
    progress_bar.empty()
    return scenes_data

# --- é–¢æ•°: éŸ³å£°æ›¸ãèµ·ã“ã— ---
@st.cache_resource
def load_whisper_model():
    # ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã®ãƒ¡ãƒ¢ãƒªåˆ¶é™å¯¾ç­–ã¨ã—ã¦ "base" ã‚’ä½¿ç”¨
    return whisper.load_model("base")

def transcribe_audio(video_path):
    model = load_whisper_model()
    with st.spinner("AIãŒéŸ³å£°ã‚’è§£æã—ã¦ã„ã¾ã™..."):
        # æ—¥æœ¬èªæŒ‡å®šã§ç²¾åº¦ã‚¢ãƒƒãƒ—
        result = model.transcribe(video_path, language="ja")
    return result["segments"]

# --- é–¢æ•°: çµåˆãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¸­ç‚¹åˆã‚ã›ï¼‰ ---
def align_scenes_and_text(scenes, segments):
    # ã‚·ãƒ¼ãƒ³ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
    for scene in scenes:
        scene["text_list"] = []

    for segment in segments:
        # ã‚»ãƒªãƒ•ã®ä¸­é–“æ™‚é–“ã‚’è¨ˆç®—
        mid_point = (segment["start"] + segment["end"]) / 2
        
        # ä¸­é–“æ™‚é–“ãŒã©ã®ã‚·ãƒ¼ãƒ³ã«å«ã¾ã‚Œã‚‹ã‹åˆ¤å®š
        matched = False
        for scene in scenes:
            if scene["start"] <= mid_point < scene["end"]:
                scene["text_list"].append(segment["text"])
                matched = True
                break
        
        # ã©ã“ã«ã‚‚å±ã•ãªã‹ã£ãŸå ´åˆï¼ˆæœ«å°¾ãªã©ï¼‰ã€æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã¸
        if not matched and scenes:
            scenes[-1]["text_list"].append(segment["text"])

    # ãƒªã‚¹ãƒˆã‚’çµåˆ
    for scene in scenes:
        scene["final_text"] = "\n".join(scene["text_list"])
    
    return scenes

# ==========================================
# ãƒ¡ã‚¤ãƒ³UI
# ==========================================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro Cloud", layout="wide")

st.title("ğŸ¥ å‹•ç”»è§£æ & ã‚¹ãƒ—ã‚·ä¸€æ‹¬è²¼ã‚Šä»˜ã‘")
st.markdown("Streamlit Cloudå¯¾å¿œç‰ˆï¼šã‚·ãƒ¼ãƒ³ç”»åƒæŠ½å‡ºã¨æ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã€Excel/ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®è²¼ã‚Šä»˜ã‘ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚")

uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (MP4æ¨å¥¨)", type=["mp4", "mov", "avi"])

if uploaded_file is not None:
    # ä¸€æ™‚ä¿å­˜
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"æº–å‚™å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary"):
        clear_output_folder()
        
        try:
            # 1. è§£æå®Ÿè¡Œ
            scenes = extract_scenes(video_path)
            segments = transcribe_audio(video_path)
            
            # 2. ãƒ‡ãƒ¼ã‚¿çµåˆ
            aligned_data = align_scenes_and_text(scenes, segments)
            
            st.divider()

            # --- A. ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º ---
            st.subheader("1. è§£æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            cols = st.columns(3)
            for i, item in enumerate(aligned_data):
                with cols[i % 3]:
                    if item["img_path"]:
                        st.image(item["img_path"], use_column_width=True)
                    st.caption(f"ã‚·ãƒ¼ãƒ³ {i+1} ({item['time_str']}~)")
                    st.text(item["final_text"])

            st.divider()

            # --- B. ã‚¹ãƒ—ã‚·è²¼ã‚Šä»˜ã‘ç”¨ãƒ‡ãƒ¼ã‚¿ ---
            st.subheader("2. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ç”¨ãƒ‡ãƒ¼ã‚¿")
            st.info("ğŸ‘‡ ä¸‹ã®ãƒœãƒƒã‚¯ã‚¹ã®å³ä¸Šã«ã‚ã‚‹ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®A1ã‚»ãƒ«ã‚’é¸æŠã—ã¦è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚æ¨ªä¸€åˆ—ã«å±•é–‹ã•ã‚Œã¾ã™ã€‚")

            # ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ (æ”¹è¡Œã¯ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã¦ã‚»ãƒ«å´©ã‚Œã‚’é˜²æ­¢)
            tsv_list = []
            for item in aligned_data:
                clean_text = item["final_text"].replace("\n", " ").replace("\t", " ")
                tsv_list.append(clean_text)
            
            tsv_string = "\t".join(tsv_list)
            
            st.code(tsv_string, language="text")
            
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

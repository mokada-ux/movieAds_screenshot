import streamlit as st
import os
import cv2
import whisper
import shutil
import datetime
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

# ===========================
# åˆæœŸè¨­å®š
# ===========================
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===========================
# Utility Functions
# ===========================
def format_time(seconds):
    seconds = int(seconds)
    minutes = seconds // 60
    rem_seconds = seconds % 60
    return f"{minutes:02}:{rem_seconds:02}"

def clear_output_folder():
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===========================
# ã‚·ãƒ¼ãƒ³æŠ½å‡º
# ===========================
def extract_scenes(video_path):
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=27.0))

    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)
    scene_list = scene_manager.get_scene_list()

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        st.error("å‹•ç”»ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    duration = frame_count / fps if fps > 0 else 0

    scenes = []

    # æœ€åˆã®ã‚·ãƒ¼ãƒ³ã‚’å¼·åˆ¶è¿½åŠ 
    if not scene_list or scene_list[0][0].get_seconds() > 1.0:
        scenes.append({
            "start": 0.0,
            "end": scene_list[0][0].get_seconds() if scene_list else duration,
            "time_str": format_time(0),
            "img_path": None
        })

    # SceneDetect ã®çµæœã‚’æ•´å½¢
    for i, scene in enumerate(scene_list):
        start = scene[0].get_seconds()
        end = scene[1].get_seconds()
        scenes.append({
            "start": start,
            "end": end,
            "time_str": format_time(start),
            "img_path": None
        })

    # -------------------------
    # ã‚·ãƒ¼ãƒ³ã”ã¨ã®ç”»åƒã‚’ä¿å­˜
    # -------------------------
    progress = st.progress(0, text="ã‚·ãƒ¼ãƒ³ç”»åƒæŠ½å‡ºä¸­...")
    total = len(scenes)

    for i, scene in enumerate(scenes):
        scene_len = scene["end"] - scene["start"]
        capture_point = scene["start"] + (0.5 if scene_len > 1.0 else 0.0)

        cap.set(cv2.CAP_PROP_POS_MSEC, capture_point * 1000)
        ret, frame = cap.read()

        if ret:
            img_path = os.path.join(OUTPUT_DIR, f"scene_{i:03d}.jpg")
            cv2.imwrite(img_path, frame)
            scene["img_path"] = img_path
        else:
            scene["img_path"] = None

        progress.progress((i + 1) / total)

    progress.empty()
    cap.release()
    return scenes

# ===========================
# Whisper éŸ³å£°æ›¸ãèµ·ã“ã—
# ===========================
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

def transcribe_audio(video_path):
    model = load_whisper_model()
    with st.spinner("AI ãŒéŸ³å£°ã‚’è§£æä¸­..."):
        result = model.transcribe(video_path, language="ja")
    return result["segments"]

# ===========================
# ã‚·ãƒ¼ãƒ³Ã—éŸ³å£°ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
# ===========================
def align_scenes_and_text(scenes, segments):
    for scene in scenes:
        scene["text_list"] = []

    for seg in segments:
        seg_start = seg["start"]
        seg_end = seg["end"]
        seg_mid = (seg_start + seg_end) / 2

        matched = False

        for scene in scenes:
            if scene["start"] <= seg_mid < scene["end"]:
                scene["text_list"].append(seg["text"])
                matched = True
                break

        if not matched and scenes:
            scenes[-1]["text_list"].append(seg["text"])

    # çµåˆ
    for scene in scenes:
        scene["final_text"] = "\n".join(scene["text_list"])

    return scenes

# ===========================
# Streamlit UI
# ===========================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro", layout="wide")
st.title("ğŸ¥ å‹•ç”»è§£æ & ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ãƒ„ãƒ¼ãƒ«ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«Whisperç‰ˆï¼‰")

uploaded_file = st.file_uploader("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi"])

if uploaded_file:
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ", type="primary"):
        clear_output_folder()

        # Step1: ã‚·ãƒ¼ãƒ³æŠ½å‡º
        scenes = extract_scenes(video_path)

        # Step2: éŸ³å£°æ›¸ãèµ·ã“ã—
        segments = transcribe_audio(video_path)

        # Step3: ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
        aligned = align_scenes_and_text(scenes, segments)

        st.divider()
        st.subheader("1. è§£æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        cols = st.columns(3)
        for i, item in enumerate(aligned):
            with cols[i % 3]:
                if item["img_path"]:
                    st.image(item["img_path"], use_column_width=True)
                st.caption(f"ã‚·ãƒ¼ãƒ³ {i+1}ï¼ˆ{item['time_str']}ã€œï¼‰")
                st.text_area("å†…å®¹", item["final_text"], height=110, key=f"text_{i}")

        st.divider()
        st.subheader("2. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ç”¨")

        tsv_text = "\t".join([item["final_text"].replace("\n", " ") for item in aligned])
        st.code(tsv_text, language="text")

import streamlit as st
import os
import shutil
import datetime
import tempfile
import whisper
import pandas as pd
from scenedetect import detect, ContentDetector
from scenedetect.video_splitter import split_video_ffmpeg

# ===============================
# è¨­å®š
# ===============================
UPLOAD_DIR = "temp_uploads"
OUTPUT_DIR = "temp_outputs"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ===============================
# é–¢æ•°
# ===============================

def format_time(seconds):
    """ç§’ â†’ 00:00:00 å½¢å¼ã¸"""
    return str(datetime.timedelta(seconds=int(seconds)))


def clear_output_folder():
    """å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€åˆæœŸåŒ–"""
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def extract_scenes_ffmpeg(video_path):
    """SceneDetect + FFmpeg ã§ã‚·ãƒ¼ãƒ³é™æ­¢ç”»ã‚’æŠ½å‡º"""
    st.info("ã‚·ãƒ¼ãƒ³æ¤œå‡ºä¸­...")

    # SceneDetect ã§ã‚·ãƒ¼ãƒ³æŠ½å‡º
    scene_list = detect(video_path, ContentDetector())

    # FFmpeg ã§ã®é™æ­¢ç”»å‡ºåŠ›ï¼ˆjpgï¼‰
    split_video_ffmpeg(
        video_path,
        scene_list,
        output_dir=OUTPUT_DIR,
        filename_template="$SCENE_NUMBER.jpg",
        format="jpg"
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«åé †ã«ä¸¦ã³æ›¿ãˆ
    images = sorted(os.listdir(OUTPUT_DIR))

    scenes_data = []
    for i, scene in enumerate(scene_list):
        start_sec = scene[0].get_seconds()
        img_file = images[i] if i < len(images) else None
        if img_file:
            scenes_data.append({
                "time_str": format_time(start_sec),
                "seconds": start_sec,
                "img_path": os.path.join(OUTPUT_DIR, img_file)
            })

    return scenes_data


@st.cache_resource
def load_whisper_model():
    """Whisper-small ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿"""
    return whisper.load_model("small")


def transcribe_audio(video_path):
    """Whisper ã§æ–‡å­—èµ·ã“ã—"""
    model = load_whisper_model()
    with st.spinner("éŸ³å£°ã‚’è§£æä¸­..."):
        result = model.transcribe(video_path, language="ja")
    return result["segments"]


def align_scenes_and_text(scenes, segments):
    """ã‚·ãƒ¼ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç´ä»˜ã‘"""
    aligned_data = []

    for i, scene in enumerate(scenes):
        scene_start = scene["seconds"]
        next_scene_start = scenes[i+1]["seconds"] if i+1 < len(scenes) else float('inf')

        matched_texts = [
            seg["text"]
            for seg in segments
            if scene_start <= seg["start"] < next_scene_start
        ]

        aligned_data.append({
            "time": scene["time_str"],
            "image": scene["img_path"],
            "text": "\n".join(matched_texts)
        })

    return aligned_data


# ===============================
# UI
# ===============================
st.set_page_config(page_title="å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro", layout="wide")

st.title("ğŸ¥ å‹•ç”»è§£æã‚¢ãƒ—ãƒª Pro")
st.markdown("Whisper-small Ã— SceneDetect(video_splitter) ã§æœ€é©åŒ–æ¸ˆã¿ã€‚Geminiç‰ˆã¨åŒç­‰ã®å‹•ä½œã€‚")

uploaded_file = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (mp4/mov ãªã©)", type=["mp4", "mov", "m4v", "avi", "webm"])

if uploaded_file is not None:
    video_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(video_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"èª­ã¿è¾¼ã¿å®Œäº†: {uploaded_file.name}")

    if st.button("ğŸš€ è§£æã‚¹ã‚¿ãƒ¼ãƒˆ"):
        clear_output_folder()

        # --- ã‚·ãƒ¼ãƒ³é™æ­¢ç”»æŠ½å‡ºï¼ˆFFmpegï¼‰ ---
        scenes = extract_scenes_ffmpeg(video_path)

        # --- Whisper ã§æ–‡å­—èµ·ã“ã— ---
        segments = transcribe_audio(video_path)

        # --- ã‚·ãƒ¼ãƒ³ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ ---
        aligned_data = align_scenes_and_text(scenes, segments)

        # -----------------------------------
        # çµæœè¡¨ç¤ºUI
        # -----------------------------------

        st.divider()
        st.subheader("ğŸ“Š è§£æçµæœï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè²¼ã‚Šä»˜ã‘ç”¨ï¼‰")

        num_scenes = len(aligned_data)

        # â± æ™‚é–“
        cols_time = st.columns(num_scenes)
        for i, col in enumerate(cols_time):
            col.write(f"**{aligned_data[i]['time']}**")

        # ğŸ–¼ ç”»åƒ
        cols_img = st.columns(num_scenes)
        for i, col in enumerate(cols_img):
            col.image(aligned_data[i]["image"], use_column_width=True)

        # ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ
        cols_text = st.columns(num_scenes)
        for i, col in enumerate(cols_text):
            col.text_area("", aligned_data[i]["text"], height=150, key=f"t_{i}")

        # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        df = pd.DataFrame(aligned_data)
        csv = df.to_csv(index=False).encode("utf-8_sig")
        st.download_button("ğŸ“¥ CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv, "video_analysis.csv")

import streamlit as st
import os
import tempfile
import subprocess
import base64
from moviepy.editor import VideoFileClip
import whisper


# ==============================
# Streamlit åŸºæœ¬è¨­å®š
# ==============================
st.set_page_config(page_title="å‹•ç”»ã‚·ãƒ¼ãƒ³è§£æãƒ„ãƒ¼ãƒ«", layout="wide")

st.markdown("""
<style>
.scene-container {
    display: flex;
    flex-wrap: nowrap;
    overflow-x: auto;
    gap: 20px;
    padding: 10px;
}
.scene-card {
    flex: 0 0 auto;
    width: 260px;
    background: #ffffff10;
    padding: 12px;
    border-radius: 12px;
    border: 1px solid #888;
}
.scene-img {
    width: 100%;
    border-radius: 8px;
    border: 1px solid #666;
}
.scene-time {
    font-size: 14px;
    margin-top: 6px;
    color: #ddd;
}
.scene-text {
    font-size: 15px;
    margin-top: 6px;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¬ å‹•ç”»ã‚·ãƒ¼ãƒ³è§£æãƒ„ãƒ¼ãƒ«ï¼ˆãƒ•ãƒ«ãƒªãƒ©ã‚¤ãƒˆç‰ˆï¼‰")


# ==============================
# ã‚·ãƒ¼ãƒ³æŠ½å‡ºï¼ˆFFmpegï¼‰
# ==============================
def extract_scenes_ffmpeg(video_path):
    tmp_dir = tempfile.mkdtemp()

    # SceneDetect + FFmpeg ã®é–¾å€¤
    threshold = "0.3"

    cmd = [
        "ffmpeg",
        "-i", video_path,
        "-vf", f"select='gt(scene,{threshold})',metadata=print",
        "-vsync", "vfr",
        os.path.join(tmp_dir, "scene_%04d.jpg")
    ]

    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    # ãƒ•ã‚¡ã‚¤ãƒ«åé †ã«ä¸¦ã¶
    image_paths = sorted(
        [os.path.join(tmp_dir, f) for f in os.listdir(tmp_dir) if f.endswith(".jpg")]
    )
    return image_paths


# ==============================
# Whisper ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# ==============================
@st.cache_resource
def load_whisper():
    return whisper.load_model("small")


def transcribe_audio(video_path):
    model = load_whisper()
    result = model.transcribe(video_path, fp16=False)
    return result["text"]


# ==============================
# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ TSVï¼ˆæ¨ª3è¡ŒÃ—nåˆ—ï¼‰
# ==============================
def generate_tsv_horizontal(image_paths, times, transcripts):
    # 1è¡Œç›®ï¼ˆæ™‚é–“ï¼‰
    time_row = ["æ™‚é–“"] + [str(t) for t in times]

    # 2è¡Œç›®ï¼ˆç”»åƒï¼‰
    image_row = ["ç”»åƒ"]
    for img in image_paths:
        with open(img, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()
        img_formula = f'=IMAGE("data:image/jpeg;base64,{b64}")'
        image_row.append(img_formula)

    # 3è¡Œç›®ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
    text_row = ["ãƒ†ã‚­ã‚¹ãƒˆ"] + transcripts

    # TSVåŒ–
    tsv = "\n".join([
        "\t".join(time_row),
        "\t".join(image_row),
        "\t".join(text_row)
    ])

    return tsv


# ==============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==============================
uploaded = st.file_uploader("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmp4 / movï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov"])

if uploaded:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(uploaded.read())
        video_path = tmp.name

    st.success("å‹•ç”»ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")

    # å‹•ç”»æƒ…å ±
    video = VideoFileClip(video_path)
    duration = video.duration
    st.write(f"å‹•ç”»é•·ã•ï¼š{duration:.1f} ç§’")

    # ã‚·ãƒ¼ãƒ³æŠ½å‡º
    with st.spinner("ã‚·ãƒ¼ãƒ³æŠ½å‡ºä¸­â€¦"):
        scene_images = extract_scenes_ffmpeg(video_path)

    st.write(f"æŠ½å‡ºã•ã‚ŒãŸã‚·ãƒ¼ãƒ³æ•°ï¼š{len(scene_images)}")

    # å„ç”»åƒã®ç§’æ•°å–å¾—ï¼ˆmoviepyï¼‰
    times = []
    for img in scene_images:
        filename = os.path.basename(img)
        idx = int(filename.replace("scene_", "").replace(".jpg", ""))
        t = (idx - 1) * 1.2  # é©å½“ã ãŒ SceneDetect ãŒç§’æ•°ã‚’å–ã‚‰ãªã„ãŸã‚è£œé–“
        times.append(round(t, 1))

    # Whisper ãƒ†ã‚­ã‚¹ãƒˆ
    with st.spinner("éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆè§£æä¸­ï¼ˆWhisper-smallï¼‰â€¦"):
        transcript = transcribe_audio(video_path)

    # ã‚·ãƒ¼ãƒ³å˜ä½ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç°¡æ˜“åˆ†å‰²ï¼‰
    transcripts = []
    chunk = len(scene_images)
    words = transcript.split()

    if chunk > 0:
        split_size = max(1, len(words) // chunk)

        for i in range(chunk):
            part = words[i * split_size:(i + 1) * split_size]
            transcripts.append(" ".join(part))


    # ==============================
    # UIï¼ˆæ¨ªã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚«ãƒ¼ãƒ‰ï¼‰
    # ==============================
    st.subheader("ğŸ” è‡ªå‹•æŠ½å‡ºã•ã‚ŒãŸã‚·ãƒ¼ãƒ³")

    html = '<div class="scene-container">'
    for img, t, tx in zip(scene_images, times, transcripts):
        with open(img, "rb") as f:
            b64 = base64.b64encode(f.read()).decode()

        html += f"""
        <div class="scene-card">
            <img class="scene-img" src="data:image/jpeg;base64,{b64}">
            <div class="scene-time">â± {t} ç§’</div>
            <div class="scene-text">{tx}</div>
        </div>
        """
    html += "</div>"

    st.markdown(html, unsafe_allow_html=True)

    # ==============================
    # TSVå‡ºåŠ›
    # ==============================
    st.subheader("ğŸ“‹ Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç”¨ï¼ˆæ¨ª3è¡Œ Ã— ã‚·ãƒ¼ãƒ³æ•°åˆ—ï¼‰")

    if st.button("TSV ã‚’ç”Ÿæˆ"):
        tsv = generate_tsv_horizontal(scene_images, times, transcripts)
        st.code(tsv, language="text")
        st.success("ã“ã®TSVã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è²¼ã‚‹ã¨ã€æ¨ªã«æ•´åˆ—ã—ã¾ã™ï¼")
